{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08e2e0a-8c41-4ad8-a0f0-5e9b70ae3081",
   "metadata": {},
   "source": [
    "# Home project task package 2\n",
    "\n",
    "In this home project, each group will be assigned a dataset of one of the 4 properties: band gap, formation energy, Poisson ratio and magnetization. In the following blocks, you will go through a demostration first to gain a feeling of the whole workflow (Task 1-6). Then you can import the datasets to the machine learning according to your learning objectives (Task 7&8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5048a2c-a7c7-4972-9930-d45e369bd133",
   "metadata": {},
   "source": [
    "# Data import and inspection\n",
    "\n",
    "For each datasets, you can check the data format by using dataframe.describe() function. The last coloumn is the target property and the rest columns are the corresponding feartures of the compounds. The features are extracted rom crystal structures of inorganic compunds using magpie (Material-Agnostic Platform for Informatics and Exploration, website: https://wolverton.bitbucket.io/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c36a36-72e9-42a2-abb3-66ea9e862337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Tc.csv')  ### here to select the input data file\n",
    "print('Dataset shape:', data.shape)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9492de-7bd5-4061-8b7b-76048aa21c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "features = list(data)[2:-1]\n",
    "properties = list(data)[-1]\n",
    "data = shuffle(data, random_state=0)\n",
    "X = data[features]\n",
    "y = data[properties]\n",
    "print('number of features:', len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99834bf-7e54-4b19-99bc-0945c12860d2",
   "metadata": {},
   "source": [
    "In machine learning (ML), besides collecting the efficient data, most of time we will also meet some problems that the features used to analyze are either redundent or inter-correlated, and we want a way to create a model that only includes the most important features. To solve such problems, we then need some methods to select the efficient features as well. There are three benefits for doing so. First, we make our model more simple to interpret. Second, we can reduce the variance of the model, and therefore the overfitting. Finally, we can reduce the computational cost (and time) of training a model. The process of identifying only the most relevant features is called “feature selection.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872d3f9-6540-438d-a988-6bfe52aff04b",
   "metadata": {},
   "source": [
    "Among most of ML model and statistical methods, random forests (RF) are often used for feature selection in a data science workflow. The reason is because the tree-based strategies used by random forests naturally ranks by how well they improve the purity of the node. This mean decrease in impurity over all trees (called gini impurity). Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features. In this tutorial, we will mainly take the advantage of RF to do the feature selection, but meanwhile some other type of feature selection method will also used for pre-screening.\n",
    "\n",
    "In the first round, we will do a rough regression model evaluation with the help of corss validation (CV) to test the performance of your selected model on the datasets using all features.\n",
    "### Task 1: A initial evaluation of your selected model on datasets with all features. Here your task is try to fill ML model code and cross validation code to get familiar with the selected ML algorithm, you can check these codes on sklearn web page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa62d7-d503-4195-bbf2-5f22b3584138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "### Task1: here try to fill ML model code and cross validation code, you can check these code on sklearn web page. \n",
    "\n",
    "# reg = RandomForestRegressor()  #fill here \n",
    "# scores = cross_val_score()  # fill here\n",
    "\n",
    "### result\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a66091-1457-47a9-9430-4596297ecba1",
   "metadata": {},
   "source": [
    "There are several methods of feature selection in practice. For the baisc feature screening, here we choose a simple api function given in Sklearn's feature_selection module called VarianceThreshold, whoose function is to remove features with low variance, which means the variation of these features are not huge according to the training data. VarianceThreshold is a simple basic method of feature selection that removes all features whose variance does not meet some threshold. By default, it removes all features that have a variance of 0, i.e. all features that take the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6477b-1bcd-43fd-8602-4665937451c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# Get features that return at least 95% of the Bernoulli random variables\n",
    "sel =  VarianceThreshold(threshold=(0.95*(1-0.95)))\n",
    "# You can see which features have been retained\n",
    "X_sel = sel.fit_transform(X)\n",
    "# Output results\n",
    "feature_index = sel.get_support()\n",
    "print('number of features after remove redundant ones:',sum(feature_index))\n",
    "\n",
    "## list of final features\n",
    "final_features=[]\n",
    "\n",
    "for i in range(len(features)):\n",
    "    if feature_index[i]==True:\n",
    "        final_features.append(features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71570a-c62c-48bc-ba76-25f5b205bd7b",
   "metadata": {},
   "source": [
    "One of the key steps in Machine learning models is to normalize the feature data, the function of normalisation are: \n",
    "\n",
    "(a) Normalisation speeds up gradient descent to find the optimal solution.\n",
    "Normalisation is often necessary if the machine learning model uses gradient descent to find the optimal solution, otherwise it will be difficult or even impossible to converge.\n",
    "\n",
    "(b) Normalisation has the potential to improve accuracy.\n",
    "\n",
    "Some classifiers need to calculate distances (Euclidean distances) between samples, e.g. KNN. If a feature has a very large range of values, then the distance calculation depends mainly on this feature, thus contradicting the actual situation (e.g. in this case the actual situation is that features with a small range of values are more important).\n",
    "\n",
    "But of course not all of the ML models need normalisation. For example, probabilistic models (tree models) do not require normalisation because they are not concerned with the values of the variables, but with the distribution of the variables and the conditional probabilities between them, e.g. decision trees, RF. Whereas optimization problems like Adaboost, SVM, LR, Knn, KMeans, they require normalisation. In case the ML model may need normalisation, here we normalized the data first.\n",
    "\n",
    "### Task 2.1: Check sklearn website and find out how to do normalisation, please return a variable with name X_sc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3267257-eb29-46ee-bc4d-5091925e0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Task 2.1: Check sklearn website and find out how to do normalisation, \n",
    "# please return a variable with name X_sc.\n",
    "scaler = StandardScaler()\n",
    "# ...\n",
    "\n",
    "### result \n",
    "print('X shape:', X_sc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789279f-b4ba-44dc-b09b-728cd221dc09",
   "metadata": {},
   "source": [
    "Then we will using the correlation heat map to analyze the inter-correlation between each feature, and we will only select the ones whcich are most representative. The Spearman rank-order correlation coefficient is a nonparametric measure of the monotonicity of the relationship between two datasets, and it does not assume that both datasets are normally distributed. Thus here we choose it as the function for calculating correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31a940-764d-445d-ac57-0200fb0f2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "corr = spearmanr(X_sc).correlation\n",
    "print(corr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1175f1a-23bc-4ba2-adb3-d490fe8e2209",
   "metadata": {},
   "source": [
    "Perform Ward’s linkage on a condensed distance matrix, get the hierarchical clustering (corr_linkage).\n",
    "\n",
    "### Task 2.2: please use the hierarchical clustering (corr_linkage) and final_features you get in task1 to make a dendrogarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd15f4-850a-48b7-94b9-03aaa29bafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "corr_linkage = hierarchy.ward(corr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "### Task 2.2: please use the hierarchical clustering (corr_linkage) \n",
    "# and final_features you get in task1 to make a dendrogarm named dendro.\n",
    "#\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "# dendro = hierarchy.dendrogram() # fill here\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d11eb-5f9a-491f-969a-992163e9a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dendro['leaves'])\n",
    "print(dendro['ivl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b017f-6bc9-4163-9af8-e5a067d70b13",
   "metadata": {},
   "source": [
    "### Task 2.3: Based on this hierarchical clustering order, please visualize the correlations and plot them out the heat map with colorbar using dendro['leaves'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d74cce-2e84-42ba-8276-ee968cb3051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "### Task 2.3: Based on this hierarchical clustering order, \n",
    "# please visualize the correlations \n",
    "# and plot out the heat map with colorbar using dendro['leaves'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0d9b9-5394-4058-8140-1aa83c4b1d88",
   "metadata": {},
   "source": [
    "We flat the corr_linkage matrix by using fcluster function. Now please think about what is this 't=2' means in the following 'cluster_ids' code. \n",
    "\n",
    "### Task 3: Please try to explain: 1. what are the meanings of 't=2' and 'distance' in the following 'cluster_ids' code; 2. The relationship between corr_likeage matrix and our final cluster_id_to_feature_ids;  3. Finally in selected_features, what are these selected features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608218be-4b3f-48f5-b31f-15d53aceb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = hierarchy.fcluster(corr_linkage, t=2, criterion='distance')\n",
    "from collections import defaultdict\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "print(cluster_id_to_feature_ids)\n",
    "print(cluster_id_to_feature_ids.values())\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "print(selected_features)\n",
    "print('number of features after correlation reduction:',len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c0f6d1-2fbe-46dd-8701-10eaa5abe0bb",
   "metadata": {},
   "source": [
    "Now with all these informative features, let's start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d8466-d180-4d62-8e6c-85ad20edc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=10, random_state=0)\n",
    "X_train=X_train[:,selected_features]\n",
    "X_test=X_test[:,selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd75cb-4f5b-4902-8f88-f422adde6744",
   "metadata": {},
   "source": [
    "Here as a example, we will show a pipeline random forest code combining SelectKBest and RandomForestRegressor. The reason to use RF here is because later in the importance analysis part we need to use RF. However, in the final step, you can also use the same procedure to select your best hyperparameter combinations for your selected ML model.\n",
    "\n",
    "SelectKBest will select features according to the k highest scores, the default value is 10 in sklearn. And in SelectKBest, we have to use f_regression to make it workable for regression problems. Then we will set some hyperparameter sets, by combining the pipeline model and cross validation methods, we can do a grid search to choose the best combination of these hyperparameters.\n",
    "\n",
    "### Task 4: Try to set and tune these hyperparameters by yourself, and select the best combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e354ccd-e0ce-4dba-ad13-667906d58cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "skb = SelectKBest(score_func=f_regression)\n",
    "est_rf = RandomForestRegressor(random_state=0)\n",
    "pipe_rf = Pipeline([('SKB', skb), ('forest', est_rf)])\n",
    "\n",
    "### Complete the lines below\n",
    "# param_grid_rf = {\n",
    "#     'forest__n_estimators':[],\n",
    "#     'forest__max_depth':[],\n",
    "#     'forest__max_features':[],\n",
    "#     'forest__min_samples_leaf':[]\n",
    "# } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3d466-71d5-4ce8-9f04-401dec939c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### Complete the lines below\n",
    "# gcv_rf = GridSearchCV() #fill here\n",
    "cv_rf.fit(X_train, y_train)\n",
    "print('\\nRandomForest:', gcv_rf.best_score_)\n",
    "print(gcv_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85933a-4b6b-48cd-bf6c-a6b3671f9fc7",
   "metadata": {},
   "source": [
    "Based on your previous hyperparameter sets, in this section we will try 1000 random initial state to find the best performance of the model. Next we will record this random intial state and use it as the data spliting parameter for train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12325b91-307e-42ba-b7b4-95481635f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here please fill the arguments with the best parameters you found in last step\n",
    "#est_rf = RandomForestRegressor(random_state=,max_features=' ',n_estimators= , max_depth= , min_samples_leaf= ) \n",
    "\n",
    "best_results=-10\n",
    "best_state=-1\n",
    "\n",
    "for i in range (1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=20, random_state=i)\n",
    "    X_train=X_train[:,selected_features]\n",
    "    X_test=X_test[:,selected_features]\n",
    "    est_rf.fit(X_train,y_train)\n",
    "    result=est_rf.score(X_test, y_test)\n",
    "    if result>best_results:\n",
    "        best_results=result\n",
    "        best_state=i\n",
    "\n",
    "print(best_results,best_state)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=20, random_state=best_state)\n",
    "X_train=X_train[:,selected_features]\n",
    "X_test=X_test[:,selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f734972-15f6-434a-8a24-8bf64874fae0",
   "metadata": {},
   "source": [
    "Now use the permutation_importance function to evaluate the importance of each feature. The permutation importance of one feature is calculated as follows. First, a baseline metric, defined by scoring (if is None, then a default scoring of estimator will be used, please refer to the permutation_importance web page in sklearn), is evaluated on a (potentially different) dataset defined by the X. Next, a feature column from the validation set is permuted and the metric is evaluated again. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column.\n",
    "\n",
    "### Task 5: check and learn how to use the permutation_importance function and generate a sorted mean importance results with name 'perm_sorted_idx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a588d0f-f7c6-43a8-8fbd-02962f5a4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "### Task 5\n",
    "\n",
    "# result = permutation_importance() # please fill here\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80e48e-a5ec-4522-b308-a41bc91ddf6a",
   "metadata": {},
   "source": [
    "With the generated sorted mean importances, let's ploted it out for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ddfed-d429-4033-832d-0aee00860e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_importance_sorted_idx = np.argsort(est_rf.feature_importances_)\n",
    "tree_indices = np.arange(0, len(est_rf.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.barh(tree_indices, est_rf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n",
    "\n",
    "ylabels=[]\n",
    "for i in range(len(tree_importance_sorted_idx)):\n",
    "    ylabels.append(final_features[i])\n",
    "\n",
    "\n",
    "ax1.set_yticklabels(ylabels)\n",
    "ax1.set_yticks(tree_indices)\n",
    "ax1.set_ylim((0, len(est_rf.feature_importances_)))\n",
    "\n",
    "labels=[]\n",
    "for i in range(len(perm_sorted_idx)):\n",
    "    labels.append(final_features[i])   \n",
    "    \n",
    "#print(labels,tree_importance_sorted_idx,est_rf.feature_importances_)\n",
    "ax2.boxplot(result.importances[perm_sorted_idx].T[-10:], vert=False, labels=labels)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a50ef63-f578-4c4d-9f26-9c05ae830ecd",
   "metadata": {},
   "source": [
    "Now with these plots, you will have a straight-forward impression on how will each feature affects the target properties. You can now select out some most important features by yourself and using them as the input for next ML model training, or you can automatically select the features with the help of 'SelectFromModel' function in sklearn. Below is a simple example for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25746819-3f35-4de9-ad0f-550d77c734a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Create a random forest regressor\n",
    "ref = RandomForestRegressor(n_estimators=10000, random_state=0, n_jobs=-1)\n",
    "\n",
    "# Train the classifier\n",
    "ref.fit(X_train, y_train)\n",
    "\n",
    "# Print the name and gini importance of each feature\n",
    "for feature, importance in zip(final_features, ref.feature_importances_):\n",
    "    print(feature, importance)\n",
    "    \n",
    "# Create a selector object that will use the random forest classifier to identify\n",
    "# features that have an importance of more than 0.15\n",
    "sfm = SelectFromModel(ref, threshold=0.05)\n",
    "\n",
    "# Train the selector\n",
    "sfm.fit(X_train, y_train)\n",
    "\n",
    "# Print the names of the most important features\n",
    "for feature_list_index in sfm.get_support(indices=True):\n",
    "    print(final_features[feature_list_index])\n",
    "    \n",
    "# Transform the data to create a new dataset containing only the most important features\n",
    "# Note: We have to apply the transform to both the training X and test X data.\n",
    "X_important_train = sfm.transform(X_train)\n",
    "X_important_test = sfm.transform(X_test)\n",
    "\n",
    "final_index = sfm.get_support(indices=True)\n",
    "print(final_index)\n",
    "print([final_features[i] for i in final_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccca753-ed74-44bb-a240-1a3329a34bf7",
   "metadata": {},
   "source": [
    "### Task 6: Now try to use the obtained trained model to predict the properties of test sets to evaulate the performance of the obtained RF model or the customized model by yourself, and make a comparation plot to visualize the differences between your predict and target result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9544956-19ae-4689-9b4d-d7b3c7afb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1a554-e040-4a7f-8687-1e5afab2250e",
   "metadata": {},
   "source": [
    "### Task 7: Tasks 1-6 have provided detailed guidance on the use of random forest algorithm,  please try to apply another type of ML algorithms (e.g. SVR, GP, NN ...) on the training of this dataset.\n",
    "\n",
    "### Task 8: With what you have learned in these tasks, please do ML on your own dataset as shown in this code.      \n",
    "\n",
    "Hint for Task 7 & 8: since the dataset's size is much larger than that of test dataset, you need to consider the size of train set and the parameters to balance the training speed and accuracy. To avoid the contingency of relatively small training sets, you can use multiple training sets and average their results(batch training is also recommended). The best performing training set is often the most representative data, and you can save it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3d455-f2fa-4c89-ade1-7be00aa77c08",
   "metadata": {},
   "source": [
    "### Task 9: Uncertainty estimation\n",
    "\n",
    "The previous 8 tasks, contains common methods for improving our machine learning models. In the process, we also found that the size and quality of the training set has a significant impact on the model performance. However, in practice, we often cannot specify a certain proportion of the whole material discovery space or certain materials as a training set as we wish. If you're an experimentalist or simulation calculator, you're probably used to this kind of scenarios: days and weeks of effort to obtain the target properties of only one material. \n",
    "\n",
    "This has led to many machine learning models that aim to make predictions about material systems struggling. So in this case do we have the opportunity to find the interested candicates in by optimizing our sampling? **Uncertainty estimation might be the key to this question.**   \n",
    "\n",
    "Uncertainty estimation allows researchers to understand the reliability of these conclusions and provides a measure of how representative the small sample is of the entire materials space. In addition, by quantifying the uncertainty, researchers can determine the range of values or variability that might exist across the materials space. This understanding is crucial for extrapolating findings and making predictions for materials that were not part of the original sample.\n",
    "\n",
    "To obtain uncertainty, **bootstrapping method** is one way, consider that we only have a small part of data randomly picked from the large dataset, first choose one machine learnng algrithm on this train set (e.g random forest) and train a model. Then we resample the data and train a new model. By doing such bootsrapped sampling for some times, each sample in the trainset is in generally equally sampled and the models obtain variety. For each candicate in the whole large dataset, there is a bunch of predictions corresponding to each bootstrapping model. The stadard deviation of these predictions provide us with the uncertainty. \n",
    "\n",
    "In this task please realize the uncertainty estimation of the prediction. You can use either the [bootstraping method](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) described above (It's both time- and resource- consuming due to the resamplings and retrainings), or any other method you have in mind. **(you will get an 0.3 bonus for your effort to save time and calculation resource!!!)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781bd35e-df22-42d9-92cf-08188134391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a55fa-bd07-4c70-99dd-d2c16cf64422",
   "metadata": {},
   "source": [
    "### Task 10: Active learning, Exporation or Optimization?\n",
    "\n",
    "From the previous Task 9 we have an idea of how to estimate the uncertainty. In our case, the uncertainty can help us both in **exploration of materials space and identifying promising candidates.**\n",
    "\n",
    "With a small sample size relative to the materials space, it is essential to develop **efficient and effective sampling strategies**. Uncertainty estimation can guide the design of these strategies by providing insights into the variability and uncertainty associated with different sampling approaches. Researchers can use the estimated uncertainties to determine the optimal allocation of resources and prioritize the sampling of materials that are likely to provide the most valuable information.\n",
    "\n",
    "Moreover, in the field of materials discovery, the identification of promising materials for specific applications is a key objective. Uncertainty estimation plays a vital role in assessing the reliability of performance predictions and property assessments for these candidate materials. By quantifying the uncertainty, researchers can rely on the level of confidence associated with the predictions, allowing them to prioritize and concentrate their efforts on **the most promising candidates for further exploration.** \n",
    "\n",
    "Then for Task 10, establishing an active learning loop is required. **YOU NEED TO CLARIFY YOUR GOAL**. Use your active learning loop to help you train a high performance ML model with a small percentage of the whole data? Or you wanna find those extremelly high performance materials in limited iterations?\n",
    " \n",
    "Some links might be helpful:\n",
    "\n",
    "https://towardsdatascience.com/active-learning-in-machine-learning-525e61be16e5\n",
    "\n",
    "https://distill.pub/2020/bayesian-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85f413-bb16-4f0e-b1c7-0be25dc01ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c358e3-88f8-47cb-b8e9-f3068434ae23",
   "metadata": {},
   "source": [
    "### Important notes:\n",
    "\n",
    "1. For Task 7&8, please provide us with your final trained models(for sklearn, save model using joblib), we will generate **5 unknown test datasets** for each properties, your final models will be evaluated on these 5 test sets. **Please note** that this is just a validation for us and don't worry if your model is not so perfect. \n",
    "\n",
    "2. For Task 9-10, please wrap up your **all** results, plots and discussions to one or several Jupyter notebooks. **Your home project will be graded based on the report.** \n",
    "\n",
    "\n",
    "3. **Updated grading criteria**:\n",
    "    \n",
    "    The group finishing Task 1-8 are guaranteed to get grade within 2.7-4.0. \\\n",
    "    The group finishing up tp Task 9 get at least 2.3. \\\n",
    "    The group finishing all the tasks get at least 1.3.\n",
    "    \n",
    "Please let your supervisor Xiankang Tang know if you have any question. (xtang@tmm.tu-darmstadt.de)\n",
    "\n",
    "Good Luck!（‐＾▽＾‐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee8cbb-8f10-4869-9549-63fa8918dacc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
